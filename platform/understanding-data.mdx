---
title: 'Understanding the Data'
description: 'Learn how to interpret and analyze the data and insights provided by BrandRadar'
---

# Understanding Key Metrics

BrandRadar aggregates data from multiple large language models. Understanding its metrics helps you interpret your brand's performance and make data-driven decisions.

<Info>
**Data Sources:** BrandRadar collects data from multiple AI platforms including OpenAI, Gemini, and Perplexity to provide comprehensive insights.
</Info>

## Key Metrics Explained

### Brand Visibility Score
Overall percentage score indicating how visible your brand is across all monitored sources. Displayed as a gauge with contributions from each LLM (OpenAI, Perplexity, Gemini). Higher scores indicate stronger brand presence.

### Variation vs. Consolidated Visibility
Two scores for competitor brands:
- **Variation**: Visibility of individual brand variants
- **Consolidated**: Combined visibility across all variants under a single brand

### Prompt Score
A percentage reflecting the importance or performance of a prompt, displayed with a progress bar. Higher scores indicate better relevance and impact on brand visibility.

### Sentiment Analysis
The ratio of positive to negative sentiment detected in responses. Shows percentage breakdown of positive, neutral, and negative sentiment for each prompt.

### Topic Score
Percentage score showing how significantly a topic contributes to brand visibility. Higher scores indicate topics that drive more brand mentions and engagement.

### Keyword Score
Similar to topic score but for individual keywords. Indicates the impact and relevance of specific keywords in driving brand visibility.

### LLM Contributions
Percentage breakdown of contributions from different language models (OpenAI, Perplexity, Gemini) for any given metric or entity. Helps you understand which AI platforms are most active for your brand.

### Prompt Mentions
Number of monitored prompts that mention your brand out of the total prompts. Includes contributions per LLM to show which platforms are most active.

### Citations & Top Domains
- **Top Domains**: Highest-cited domains with citation counts per LLM
- **Citations**: Individual URLs with associated domains and total prompt references
- **Source Quality**: Understanding where your data comes from helps assess credibility

### Rankings
Rankings of prompts, topics, brands, or keywords based on scores and counts. Available across multiple pages to help identify top performers.

### Update Timestamps
- **Last Updated**: When a prompt was last processed
- **Next Update**: When the next data collection is scheduled
- **Plan Usage**: Number of prompts used vs. total allowed in your plan

## Complete Metrics Summary

| Metric | Purpose/Explanation |
|--------|-------------------|
| **Brand Visibility Score** | Overall percentage score indicating brand presence across all monitored sources. Shown as a gauge with LLM contributions |
| **Variation vs. Consolidated Visibility** | Two scores for competitor brands: individual brand variants vs. combined visibility across variants |
| **Prompt Mentions** | Number of monitored prompts mentioning your brand out of total prompts, with LLM breakdowns |
| **Top Domains / Citations** | Domains and URLs most frequently referenced in monitored data |
| **Prompt Score** | Percentage reflecting prompt importance/performance, displayed with progress bar |
| **Sentiment** | Sentiment analysis showing percentage of positive/neutral/negative sentiment |
| **Topic Score** | Percentage showing how significantly a topic contributes to brand visibility |
| **Keyword Score** | Similar to topic score but for individual keywords, indicating keyword impact |
| **LLM Contributions** | Percentage breakdown of contributions from different language models |
| **Rankings** | Rankings of prompts, topics, brands, or keywords based on scores and counts |
| **Update Timestamps** | When prompts were last updated and scheduled for next collection |
| **Plan and Usage Metrics** | Number of prompts used vs. total allowed and billing cycle information |

<AccordionGroup>
  <Accordion icon="chart-line" title="Interpreting Metrics">
    **Key Performance Indicators:**
    - **Prompt Score**: Higher scores = more relevant data
    - **Sentiment Ratio**: Positive/negative sentiment balance
    - **Visibility Score**: Overall brand presence ranking
    - **LLM Distribution**: Data source diversity
    
    <Tip>Review these metrics regularly to refine your prompts and topics.</Tip>
  </Accordion>
  
  <Accordion icon="balance-scale" title="Data Quality Assessment">
    **Quality Indicators:**
    - Citation diversity and credibility
    - LLM response consistency
    - Sentiment confidence levels
    - Topic relevance scores
    
    <Warning>Low-quality data may indicate the need to refine your prompts or topics.</Warning>
  </Accordion>
</AccordionGroup>


# Best Practices

## 7.1 Use Filters Liberally

Most pages have date pickers and search boxes. Filtering by date range or keyword helps you focus on specific campaigns or time periods.

## 7.2 Leverage AI-Generated Suggestions

The **Suggested prompts** feature can surface questions you might not think of. Reviewing them periodically keeps your monitoring fresh.

## 7.3 Keep Brand and Competitor Details Up to Date

Accurate information ensures the AI identifies the right entities and ignores irrelevant mentions.

## 7.4 Monitor Citations

By examining domains and links, you can gauge the credibility and diversity of your AI's sources.

## 7.5 Document Your Workflow

If you are integrating this guide into a documentation platform, link related pages (e.g., separate articles on setting up competitors or analysing topics) to help users navigate the docs easily.

<AccordionGroup>
  <Accordion icon="filter" title="Effective Filtering">
    **Filter Strategies:**
    - Use date ranges to focus on specific campaigns
    - Filter by keywords to narrow results
    - Apply LLM filters to compare model performance
    - Use sentiment filters for targeted analysis
    
    <Tip>Combine multiple filters for precise data analysis.</Tip>
  </Accordion>
  
  <Accordion icon="lightbulb" title="AI Suggestions">
    **Leveraging AI Recommendations:**
    - Regularly review suggested prompts
    - Use AI insights to discover new topics
    - Adapt suggestions to your specific needs
    - Test and iterate on AI recommendations
    
    <Note>AI suggestions are based on current trends and may evolve over time.</Note>
  </Accordion>
  
  <Accordion icon="refresh" title="Data Maintenance">
    **Keeping Data Current:**
    - Update brand variations regularly
    - Refresh competitor information
    - Review and clean up outdated topics
    - Validate citation sources
    
    <Warning>Outdated information can lead to inaccurate insights and poor decision-making.</Warning>
  </Accordion>
</AccordionGroup>

